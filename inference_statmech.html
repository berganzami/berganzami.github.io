<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html> <head>
<!---    <script src="jquery.js"></script> ---> 
<script type="text/javascript" src="http://code.jquery.com/jquery-1.4.2.min.js"></script>
    <script> 
    $(function(){
      $("#includedContent").load("menu.html"); 
    });
    </script> 
  </head> 


<body>



<div id="includedContent"></div>

<h2> Graduation degree course, "Elements of statistical inference and statistical physics"  </h2> 

<h3> Università degli Studi di Roma, "La Sapienza", corso accademico 2019-2020. </h3>

<h3> University of Granada, PhD and Master Program "Fisymat" (2019). </h3>

<h3> Motivation: </h3>

<p>>>Éste, no lo olvidemos, era casi incapaz de ideas generales, platónicas. No sólo le costaba comprender que el símbolo genérico perro abarcara tantos individuos dispares de diversos tamaños y diversa forma; le molestaba que el perro de las tres y catorce (visto de perfil) tuviera el mismo nombre que el perro de las tres y cuarto (visto de frente). Su propia cara en el espejo, sus propias manos, lo sorprendían cada vez. [...] <br>
>>Refiere Swift que el emperador de Lilliput discernía el movimiento del minutero; Funes discernía continuamente los tranquilos avances de la corrupción, de las caries, de la fatiga. Notaba los progresos de la muerte, de la humedad. Era el solitario y lúcido espectador de un mundo multiforme, instantáneo y casi intolerablemente preciso. <br>
>>Había aprendido sin esfuerzo el inglés, el francés, el portugués, el latín. Sospecho, sin embargo, que no era muy capaz de pensar. Pensar es olvidar diferencias, es generalizar, abstraer. En el abarrotado mundo de Funes no había sino detalles [...].</p>
J. L. Borges, Funes el memorioso, 1944. </p>

<p> These references may be motivating too: <a href="https://www.tandfonline.com/doi/abs/10.1080/09658211.2016.1160126?journalCode=pmem20" target="_blank"> Le Port et al 2017</a>,   <a href="https://science.sciencemag.org/content/355/6324/507" target="_blank"> de Vivo et al, 2017</a>, <a href="https://www.nature.com/articles/304111a0" target="_blank"> Crick and Mitchison, 1983</a>, <a href="https://www.cs.toronto.edu/~hinton/absps/cogsci14.pdf" target="_blank"> Hinton 2013</a>. C.f. also <a href="https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/" target="_blank">this in Wired</a>. 


</p>


<h3> Programma: </h3>

<p>1. Problema diretto e problema inverso [6 ore]</p>
<p>	- introduzione: problemi diretto e inverso. L'apprendimento non-supervisionato. I concetti di correlazioni spurie, overfitting, trade-off accuratezza/complessità. </p>
<p>	- il problema diretto in tre esempi. Campionamento da una distribuzione di probabilità univariata. Campionamento da una distribuzione di probabilità multivariata. Inefficienza del campionamento Monte Carlo uniforme da una distribuzione di probabilità multivariata ed equivalenza degli insiemi in fisica statistica. Nozioni sul campionamento Markov Chain Monte Carlo (MCMC). Definizione di entropia relativa. Energia libera variazionale ed entropia relativa. Correlazioni nel modello di Ising e nel Modello Gaussiano. </p>
<p>	- il problema inverso. Indicatori Bayesiani, massimo a-posteriori (MAP) e massima verosimiglianza (maximum likelihood, ML).  Inferenza dei parametri di una distribuzione di probabilità gaussiana univariata. Concetto di statistica sufficiente. [? Inferenza da una miscela di gaussiane.] Inferenza dei parametri di una distribuzione di probabilità multivariata. Il metodo dell'ascesa del gradiente. Correlazioni e interazioni in fisica statistica, l'espansione ad alta temperatura. Correlazioni e cumulanti. Nozioni sul metodo della massima entropia (MaxEnt). Due esempi: attività neuronale e moto animale collettivo. Nozioni sull'apprendimento non-supervisionato in reti neurali artificiali (ANN).</p>
<p>	- selezione di modelli (model selection). Accuratezza e complessità. La model evidence bayesiana: il fattore di Occam (Occam's factor) e il Criterio di Informazione Bayesiana (Bayes Information Criterion, BIC). Due esempi di selezione di modelli in inferenza: in inferenza causale e in MaxEnt (moto animale collettivo e connettività cerebrale funzionale).</p>
<p>2. Qualche nozione sugli approcci probabilistici alla cognizione [2 ore]</p>
<p>	- introduzione: il cervello come ente inferente in due esempi (clusterizzazione e ricognizione di caratteri scritti a mano). </p>
<p>	- illustrazione dell'inferenza percettuale ottima (ML) da parte di soggetti umani: integrazione di indizi sensoriali.</p>
<p>	- illustrazione delle abilità cognitive di selezione di modelli</p>
<p>	- nozioni sul principio dell'energia libera (free energy principle) per la percezione (perceptual inference).</p>
<p>3. Nozioni sull'apprendimento in reti neurali artificiali [2 ore]</p>
<p>- i modelli Boltzmann Machine e Restricted Boltzmann Machine (RBM) di rete neurale artificiale. Energia libera variazionale rivisitata: la soluzione del problema diretto di un modello interagente. Relazione tra correlazioni e interazioni nel problema diretto. Equazioni dell'apprendimento per ascesa del gradiente in RBM. Gli algoritmi Contrastive Divergence e Persistent Contrastive Divergence. </p>
<p>4. Nozioni approfondite sulle RBM [2 ore]</p>
<p>- metodo dell'ascesa del gradiente stocastico e deterministico (deterministic and stochastic gradient ascent). Interazioni effettive indotte da una RBM. Concetto di pseudo-lieklihood e calcolo della pseudo-likelihood in RBM.</p>



<br>

<h3> The material: </h3>

<br>
<li>Find <a href="files/inference.pdf" target="_blank">here<a/> the lecture notes (attention: they are updated in real-time !).</li>
<br> (you may need to reload your browser). <br>
<br>
<li>Find <a href="files/intro.pdf" target="_blank">here<a/> the motivation slides.</li>
<br>
<br>
<br>
<br>
<br>


<body>
</html>
